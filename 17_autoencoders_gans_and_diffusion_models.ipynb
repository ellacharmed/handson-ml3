{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 17 – Autoencoders, GANs, and Diffusion Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 17._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-ml3/blob/main/17_autoencoders_gans_and_diffusion_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml3/blob/main/17_autoencoders_gans_and_diffusion_models.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFXIv9qNpKzt",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IPbJEmZpKzu"
   },
   "source": [
    "This project requires Python 3.7 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFSU3FCOpKzu"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAlKky09pKzv"
   },
   "source": [
    "It also requires Scikit-Learn ≥ 1.0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqCwW7cMpKzw"
   },
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJtVEqxfpKzw"
   },
   "source": [
    "And TensorFlow ≥ 2.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Piq5se2pKzx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDaDoLQTpKzx"
   },
   "source": [
    "As we did in earlier chapters, let's define the default font sizes to make the figures prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d4TH3NbpKzx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcoUIRsvpKzy"
   },
   "source": [
    "And let's create the `images/generative` folder (if it doesn't already exist), and define the `save_fig()` function which is used through this notebook to save the figures in high-res for the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQFH5Y9PpKzy"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"generative\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTsawKlapKzy"
   },
   "source": [
    "This chapter can be very slow without a GPU, so let's make sure there's one, or else issue a warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ekxzo6pOpKzy"
   },
   "outputs": [],
   "source": [
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if \"kaggle_secrets\" in sys.modules:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing PCA with an Undercomplete Linear Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the Autoencoder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "encoder = tf.keras.Sequential([tf.keras.layers.Dense(2)])\n",
    "decoder = tf.keras.Sequential([tf.keras.layers.Dense(3)])\n",
    "autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate the same 3D dataset as we used in Chapter 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – builds the same 3D dataset as in Chapter 8\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "m = 60\n",
    "X = np.zeros((m, 3))  # initialize 3D dataset\n",
    "np.random.seed(42)\n",
    "angles = (np.random.rand(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\n",
    "X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\n",
    "X += 0.28 * np.random.randn(m, 3)  # add more noise\n",
    "X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n",
    "X_train = X + [0.2, 0, 0.2]  # shift a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(X_train, X_train, epochs=500, verbose=False)\n",
    "codings = encoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "plt.plot(codings[:,0], codings[:, 1], \"b.\")\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "save_fig(\"linear_autoencoder_pca_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementing a Stacked Autoencoder Using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the fashion MNIST dataset, scale it, and split it into a training set, a validation set, and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – loads, scales, and splits the fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build and train a stacked Autoencoder with 3 hidden layers and 1 output layer (i.e., 2 stacked Autoencoders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "stacked_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "])\n",
    "stacked_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")                   \n",
    "history = stacked_ae.fit(X_train, X_train, epochs=20,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edq73sa5yLhC",
    "tags": []
   },
   "source": [
    "## Visualizing the Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes a few validation images through the autoencoder and displays the original images and their reconstructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_reconstructions(model, images=X_valid, n_images=5):\n",
    "    reconstructions = np.clip(model.predict(images[:n_images]), 0, 1)\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plt.imshow(images[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plt.imshow(reconstructions[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plot_reconstructions(stacked_ae)\n",
    "save_fig(\"reconstruction_plot\")  # extra code – saves the high res figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstructions look fuzzy, but remember that the images were compressed down to just 30 numbers, instead of 784."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edq73sa5yLhC",
    "tags": []
   },
   "source": [
    "## Visualizing the Fashion MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g69nLh2GyLhC",
    "outputId": "47f84d4f-b41f-4ef9-d653-d73506cb4839"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
    "tsne = TSNE(init=\"pca\", learning_rate=\"auto\", random_state=42)\n",
    "X_valid_2D = tsne.fit_transform(X_valid_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klppwl4gyLhC",
    "outputId": "b66330ea-5d49-4153-82ce-39fb78614a64"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs95C-AtyLhC"
   },
   "source": [
    "Let's make this diagram a bit prettier (adapted from [this Scikit-Learn example](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIkuJSpPyLhC",
    "outputId": "239e4fe5-7756-40c1-cc2c-70c83eee6c0f"
   },
   "outputs": [],
   "source": [
    "# extra code – beautifies the previous diagram for the book\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = plt.cm.tab10\n",
    "Z = X_valid_2D\n",
    "Z = (Z - Z.min()) / (Z.max() - Z.min())  # normalize to the 0-1 range\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=y_valid, s=10, cmap=cmap)\n",
    "image_positions = np.array([[1., 1.]])\n",
    "for index, position in enumerate(Z):\n",
    "    dist = ((position - image_positions) ** 2).sum(axis=1)\n",
    "    if dist.min() > 0.02: # if far enough from other images\n",
    "        image_positions = np.r_[image_positions, [position]]\n",
    "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
    "            mpl.offsetbox.OffsetImage(X_valid[index], cmap=\"binary\"),\n",
    "            position, bboxprops={\"edgecolor\": cmap(y_valid[index]), \"lw\": 2})\n",
    "        plt.gca().add_artist(imagebox)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "save_fig(\"fashion_mnist_visualization_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OJPbzoNyLhD"
   },
   "source": [
    "## Tying weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLMREMfUyLhD"
   },
   "source": [
    "It is common to tie the weights of the encoder and the decoder, by simply using the transpose of the encoder's weights as the decoder weights. For this, we need to use a custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTru0q0CyLhD"
   },
   "outputs": [],
   "source": [
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense = dense\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name=\"bias\",\n",
    "                                      shape=self.dense.input_shape[-1],\n",
    "                                      initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "        return self.activation(Z + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_mU3e6IyLhD",
    "outputId": "e8045bff-7004-4940-8d00-295ad99c0f23"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "dense_1 = tf.keras.layers.Dense(100, activation=\"relu\")\n",
    "dense_2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "\n",
    "tied_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    dense_1,\n",
    "    dense_2\n",
    "])\n",
    "\n",
    "tied_decoder = tf.keras.Sequential([\n",
    "    DenseTranspose(dense_2, activation=\"relu\"),\n",
    "    DenseTranspose(dense_1),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "tied_ae = tf.keras.Sequential([tied_encoder, tied_decoder])\n",
    "\n",
    "# extra code – compiles and fits the model\n",
    "tied_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = tied_ae.fit(X_train, X_train, epochs=10,\n",
    "                      validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WS2bm471yLhD",
    "outputId": "8427f9cc-630f-4cc9-a1f5-7361c5eab6dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extra code – plots reconstructions\n",
    "plot_reconstructions(tied_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucUS-1GwyLhE"
   },
   "source": [
    "## Extra Material – Training one Autoencoder at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIqJ922pyLhE"
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(n_neurons, X_train, X_valid, n_epochs=10,\n",
    "                      output_activation=None):\n",
    "    n_inputs = X_train.shape[-1]\n",
    "    encoder = tf.keras.layers.Dense(n_neurons, activation=\"relu\")\n",
    "    decoder = tf.keras.layers.Dense(n_inputs, activation=output_activation)\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "    autoencoder.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "    autoencoder.fit(X_train, X_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid, X_valid))\n",
    "    return encoder, decoder, encoder(X_train), encoder(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3TSUkv4yLhE",
    "outputId": "bca2fb96-7ddf-4457-b1df-30cdce8d803e"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "X_train_flat = tf.keras.layers.Flatten()(X_train)\n",
    "X_valid_flat = tf.keras.layers.Flatten()(X_valid)\n",
    "enc1, dec1, X_train_enc1, X_valid_enc1 = train_autoencoder(\n",
    "    100, X_train_flat, X_valid_flat)\n",
    "enc2, dec2, _, _ = train_autoencoder(\n",
    "    30, X_train_enc1, X_valid_enc1, output_activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpjUrD77yLhE"
   },
   "outputs": [],
   "source": [
    "stacked_ae_1_by_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    enc1, enc2, dec2, dec1,\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbEe4tQqyLhE",
    "outputId": "52eb1adf-f4c4-4029-f68b-1ef420a5daed"
   },
   "outputs": [],
   "source": [
    "plot_reconstructions(stacked_ae_1_by_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, we can then continue training the full stacked autoencoder for a few epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRgEnA-WyLhF",
    "outputId": "3d367c4e-82d4-48d9-8757-b106edf00650"
   },
   "outputs": [],
   "source": [
    "stacked_ae_1_by_1.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = stacked_ae_1_by_1.fit(X_train, X_train, epochs=5,\n",
    "                                validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln1j0u4eyLhF",
    "outputId": "fef0981b-6b8d-4e9d-e837-9e3c20358e9e"
   },
   "outputs": [],
   "source": [
    "plot_reconstructions(stacked_ae_1_by_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGixWVtuyLhF"
   },
   "source": [
    "## Convolutional Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVK4vETzyLhF"
   },
   "source": [
    "Let's build a stacked Autoencoder with 3 hidden layers and 1 output layer (i.e., 2 stacked Autoencoders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qom8ZHdgyLhF",
    "outputId": "45b2544f-0812-4fc7-a4e5-d8df9f193f5f"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "conv_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape([28, 28, 1]),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 × 14 x 16\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 × 7 x 32\n",
    "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 3 × 3 x 64\n",
    "    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAvgPool2D()  # output: 30\n",
    "])\n",
    "conv_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3 * 3 * 16),\n",
    "    tf.keras.layers.Reshape((3, 3, 16)),\n",
    "    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\",\n",
    "                                    activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n",
    "\n",
    "# extra code – compiles and fits the model\n",
    "conv_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = conv_ae.fit(X_train, X_train, epochs=10,\n",
    "                      validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6p0Yud0RyLhG",
    "outputId": "7112e556-b714-4645-af12-3bb3bd7cd6ce"
   },
   "outputs": [],
   "source": [
    "# extra code – shows the reconstructions\n",
    "plot_reconstructions(conv_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKuHGp1DyLhG"
   },
   "source": [
    "# Extra Material – Recurrent Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's treat each Fashion MNIST image as a sequence of 28 vectors, each with 28 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGOuVy0kyLhG"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "recurrent_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(30)\n",
    "])\n",
    "recurrent_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.RepeatVector(28),\n",
    "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
    "    tf.keras.layers.Dense(28)\n",
    "])\n",
    "recurrent_ae = tf.keras.Sequential([recurrent_encoder, recurrent_decoder])\n",
    "recurrent_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4grialnWyLhG",
    "outputId": "e6bb8d79-9a57-4fd3-e336-b81687de6781"
   },
   "outputs": [],
   "source": [
    "history = recurrent_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLOnTctFyLhG",
    "outputId": "e5b59e40-09bc-4d82-bb6d-58ae7d7cc376"
   },
   "outputs": [],
   "source": [
    "plot_reconstructions(recurrent_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JafadovRyLhH"
   },
   "source": [
    "# Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdMv5zGbyLhH"
   },
   "source": [
    "Using dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTKWjK3EyLhH",
    "outputId": "3be1a3bc-7f30-4793-955e-07bf60fc3b6e"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "dropout_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "])\n",
    "dropout_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "dropout_ae = tf.keras.Sequential([dropout_encoder, dropout_decoder])\n",
    "\n",
    "# extra code – compiles and fits the model\n",
    "dropout_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = dropout_ae.fit(X_train, X_train, epochs=10,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pucj7MI3yLhI",
    "outputId": "d9a4289a-fde7-4e3b-c968-72421ddbfe6d"
   },
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 17–9\n",
    "tf.random.set_seed(42)\n",
    "dropout = tf.keras.layers.Dropout(0.5)\n",
    "plot_reconstructions(dropout_ae, dropout(X_valid, training=True))\n",
    "save_fig(\"dropout_denoising_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can try replacing the `Dropout` layer with `tf.keras.layers.GaussianNoise(0.2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd8xb5g-yLhI"
   },
   "source": [
    "# Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDseIwd4yLhJ"
   },
   "source": [
    "Let's use the sigmoid activation function in the coding layer. Let's also add $\\ell_1$ regularization to it: to do this, we add an `ActivityRegularization` layer after the coding layer. Alternatively, we could add `activity_regularizer=tf.keras.regularizers.l1(1e-4)` to the coding layer itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORrSHhNxyLhJ",
    "outputId": "664ca147-e8f2-4ccf-960e-a86e60c2c60d"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "sparse_l1_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(300, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.ActivityRegularization(l1=1e-4)\n",
    "])\n",
    "sparse_l1_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "sparse_l1_ae = tf.keras.Sequential([sparse_l1_encoder, sparse_l1_decoder])\n",
    "\n",
    "# extra code – compiles and fits the model\n",
    "sparse_l1_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = sparse_l1_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vU9twBAyLhK",
    "outputId": "feb78185-b67e-4775-c61e-3d4a8d082c75"
   },
   "outputs": [],
   "source": [
    "# extra code – shows the reconstructions\n",
    "plot_reconstructions(sparse_l1_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3vxPDGUyLhK"
   },
   "source": [
    "Let's plot the KL Divergence loss, versus the MAE and MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0fOy7aMyLhK",
    "outputId": "35a609c2-6d23-462f-a519-6d93b07d0ec4"
   },
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 17–10\n",
    "p = 0.1\n",
    "q = np.linspace(0.001, 0.999, 500)\n",
    "kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))\n",
    "mse = (p - q) ** 2\n",
    "mae = np.abs(p - q)\n",
    "plt.plot([p, p], [0, 0.3], \"k:\")\n",
    "plt.text(0.05, 0.32, \"Target\\nsparsity\", fontsize=14)\n",
    "plt.plot(q, kl_div, \"b-\", label=\"KL divergence\")\n",
    "plt.plot(q, mae, \"g--\", label=r\"MAE ($\\ell_1$)\")\n",
    "plt.plot(q, mse, \"r--\", linewidth=1, label=r\"MSE ($\\ell_2$)\")\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.xlabel(\"Actual sparsity\")\n",
    "plt.ylabel(\"Cost\", rotation=0)\n",
    "plt.axis([0, 1, 0, 0.95])\n",
    "plt.grid(True)\n",
    "save_fig(\"sparsity_loss_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a custom regularizer for KL-Divergence regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-MoRrQGyLhK"
   },
   "outputs": [],
   "source": [
    "kl_divergence = tf.keras.losses.kullback_leibler_divergence\n",
    "\n",
    "class KLDivergenceRegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, weight, target):\n",
    "        self.weight = weight\n",
    "        self.target = target\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        mean_activities = tf.reduce_mean(inputs, axis=0)\n",
    "        return self.weight * (\n",
    "            kl_divergence(self.target, mean_activities) +\n",
    "            kl_divergence(1. - self.target, 1. - mean_activities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this regularizer to push the model to have about 10% sparsity in the coding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIoQ-1D4yLhK",
    "outputId": "8874ead0-468a-47f8-9e35-4a855bace6b4"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "kld_reg = KLDivergenceRegularizer(weight=5e-3, target=0.1)\n",
    "sparse_kl_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(300, activation=\"sigmoid\",\n",
    "                          activity_regularizer=kld_reg)\n",
    "])\n",
    "sparse_kl_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "sparse_kl_ae = tf.keras.Sequential([sparse_kl_encoder, sparse_kl_decoder])\n",
    "\n",
    "# extra code – compiles and fits the model\n",
    "sparse_kl_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = sparse_kl_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0jIPg2ayLhL",
    "outputId": "35512003-0640-4b9a-835b-6c7af95c930f"
   },
   "outputs": [],
   "source": [
    "# extra code – shows the reconstructions\n",
    "plot_reconstructions(sparse_kl_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1WIZRNByLhL"
   },
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64ji8G6eyLhL"
   },
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "codings_size = 10\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=[28, 28])\n",
    "Z = tf.keras.layers.Flatten()(inputs)\n",
    "Z = tf.keras.layers.Dense(150, activation=\"relu\")(Z)\n",
    "Z = tf.keras.layers.Dense(100, activation=\"relu\")(Z)\n",
    "codings_mean = tf.keras.layers.Dense(codings_size)(Z)  # μ\n",
    "codings_log_var = tf.keras.layers.Dense(codings_size)(Z)  # γ\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "variational_encoder = tf.keras.Model(\n",
    "    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tf.keras.layers.Input(shape=[codings_size])\n",
    "x = tf.keras.layers.Dense(100, activation=\"relu\")(decoder_inputs)\n",
    "x = tf.keras.layers.Dense(150, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(28 * 28)(x)\n",
    "outputs = tf.keras.layers.Reshape([28, 28])(x)\n",
    "variational_decoder = tf.keras.Model(inputs=[decoder_inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, codings = variational_encoder(inputs)\n",
    "reconstructions = variational_decoder(codings)\n",
    "variational_ae = tf.keras.Model(inputs=[inputs], outputs=[reconstructions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_loss = -0.5 * tf.reduce_sum(\n",
    "    1 + codings_log_var - tf.exp(codings_log_var) - tf.square(codings_mean),\n",
    "    axis=-1)\n",
    "variational_ae.add_loss(tf.reduce_mean(latent_loss) / 784.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXUKW_UPyLhL",
    "outputId": "6db5b948-1b3d-419b-e810-08512fa65369"
   },
   "outputs": [],
   "source": [
    "variational_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,\n",
    "                             validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDs525puyLhL",
    "outputId": "049794c0-4407-49b7-ba65-138f31d5e5dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_reconstructions(variational_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJfl2bXbyLhM"
   },
   "source": [
    "## Generate Fashion Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a few random codings and decode them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "codings = tf.random.normal(shape=[3 * 7, codings_size])\n",
    "images = variational_decoder(codings).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot these images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzwLpDIDyLhM"
   },
   "outputs": [],
   "source": [
    "# extra code – this cells generates and saves Figure 17-12\n",
    "\n",
    "def plot_multiple_images(images, n_cols=None):\n",
    "    n_cols = n_cols or len(images)\n",
    "    n_rows = (len(images) - 1) // n_cols + 1\n",
    "    if images.shape[-1] == 1:\n",
    "        images = images.squeeze(axis=-1)\n",
    "    plt.figure(figsize=(n_cols, n_rows))\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plot_multiple_images(images, 7)\n",
    "save_fig(\"vae_generated_images_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JLZ_HWqyLhM"
   },
   "source": [
    "Now let's perform semantic interpolation between 2 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsVXES9byLhM",
    "outputId": "eca5be95-9eee-47d1-e9ce-50756e24d235"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "codings = np.zeros([7, codings_size])\n",
    "codings[:, 3] = np.linspace(-0.8, 0.8, 7)  # axis 3 looks best in this case\n",
    "images = variational_decoder(codings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 17–13\n",
    "plot_multiple_images(images)\n",
    "save_fig(\"semantic_interpolation_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hA5xhT_yLhN"
   },
   "source": [
    "# Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N31KxgksyLhN"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "codings_size = 30\n",
    "\n",
    "Dense = tf.keras.layers.Dense\n",
    "generator = tf.keras.Sequential([\n",
    "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "gan = tf.keras.Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qm4Uk3MTyLhN"
   },
   "outputs": [],
   "source": [
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQoPjJthyLhN"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxrUUhrjyLhN",
    "outputId": "2813bb7d-84c4-4ed2-8d03-c1fbd138c576"
   },
   "outputs": [],
   "source": [
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")  # extra code\n",
    "        for X_batch in dataset:\n",
    "            # phase 1 - training the discriminator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            # phase 2 - training the generator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            gan.train_on_batch(noise, y2)\n",
    "        # extra code — plot images during training\n",
    "        plot_multiple_images(generated_images.numpy(), 8)\n",
    "        plt.show()\n",
    "\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
    "generated_images = generator.predict(codings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrLzMxweyLhO",
    "outputId": "b21da95e-da35-45b6-bbee-fd6e8b915382"
   },
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 17–15\n",
    "plot_multiple_images(generated_images, 8)\n",
    "save_fig(\"gan_generated_images_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZN7cSsKyLhO"
   },
   "source": [
    "# Deep Convolutional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgT77mdVyLhO"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "codings_size = 100\n",
    "\n",
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(7 * 7 * 128),\n",
    "    tf.keras.layers.Reshape([7, 7, 128]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"tanh\"),\n",
    "])\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
    "                        activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n",
    "                        activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "gan = tf.keras.Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-Sj_zc9yLhO"
   },
   "outputs": [],
   "source": [
    "# extra code – compiles the discrimator and the gan, as earlier\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ss7zwLGyLhO"
   },
   "outputs": [],
   "source": [
    "X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKWHUiUqyLhP",
    "outputId": "576b04b2-5604-405a-a672-9693db6ef26a"
   },
   "outputs": [],
   "source": [
    "# extra code – generates the dataset and trains the GAN, just like earlier\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY8_xytVyLhP",
    "outputId": "7f3ebd45-3bc5-42ce-d0a6-5734d360cef6"
   },
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 17–16\n",
    "tf.random.set_seed(42)\n",
    "noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "generated_images = generator.predict(noise)\n",
    "plot_multiple_images(generated_images, 8)\n",
    "save_fig(\"dcgan_generated_images_plot\", tight_layout=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUCR2FrdyLhP"
   },
   "source": [
    "# Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyvKvczoil-7"
   },
   "source": [
    "Starting with an image from the dataset, at each time step $t$, the diffusion process adds Gaussian noise with mean 0 and variance $\\beta_t$. The model is then trained to reverse that process. More specifically, given a noisy image produced by the forward process, and given the time $t$, the model is trained to predict the total noise that was added to the original image, scaled to variance 1.\n",
    "\n",
    "The [DDPM paper](https://arxiv.org/abs/2006.11239) increased $\\beta_t$ from $\\beta_1$ = 0.0001 to $\\beta_T = $0.02 ($T$ is the max step), but the [Improved DDPM paper](https://arxiv.org/pdf/2102.09672.pdf) suggested using the following $\\cos^2(\\ldots)$ schedule instead, which gradually decreases $\\bar{\\alpha_t} = \\prod_{i=0}^{t} \\alpha_i$ from 1 to 0, where $\\alpha_t = 1 - \\beta_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxPNTP3QpHAw"
   },
   "outputs": [],
   "source": [
    "def variance_schedule(T, s=0.008, max_beta=0.999):\n",
    "    t = np.arange(T + 1)\n",
    "    f = np.cos((t / T + s) / (1 + s) * np.pi / 2) ** 2\n",
    "    alpha = np.clip(f[1:] / f[:-1], 1 - max_beta, 1)\n",
    "    alpha = np.append(1, alpha).astype(np.float32)  # add α₀ = 1\n",
    "    beta = 1 - alpha\n",
    "    alpha_cumprod = np.cumprod(alpha)\n",
    "    return alpha, alpha_cumprod, beta  # αₜ , α̅ₜ , βₜ for t = 0 to T\n",
    "\n",
    "np.random.seed(42)  # extra code – for reproducibility\n",
    "T = 4000\n",
    "alpha, alpha_cumprod, beta = variance_schedule(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4ijMDaVkyXi"
   },
   "source": [
    "In the DDPM paper, the authors used $T = 1,000$, while in the Improved DDPM, they bumped this up to $T = 4,000$, so we use this value. The variable `alpha` is a vector containing $\\alpha_0, \\alpha_1, ..., \\alpha_T$. The variable `alpha_cumprod` is a vector containing $\\bar{\\alpha_0}, \\bar{\\alpha_1}, ..., \\bar{\\alpha_T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z6rfMntnSMi"
   },
   "source": [
    "Let's plot `alpha_cumprod`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 17–21\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(beta, \"r--\", label=r\"$\\beta_t$\")\n",
    "plt.plot(alpha_cumprod, \"b\", label=r\"$\\bar{\\alpha}_t$\")\n",
    "plt.axis([0, T, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.xlabel(r\"t\")\n",
    "plt.legend()\n",
    "save_fig(\"variance_schedule_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL7ZBkTKndy-"
   },
   "source": [
    "The `prepare_batch()` function takes a batch of images and adds noise to each of them, using a different random time between 1 and $T$ for each image, and it returns a tuple containing the inputs and the targets:\n",
    "\n",
    "* The inputs are a `dict` containing the noisy images and the corresponding times. The function uses equation (4) from the DDPM paper to compute the noisy images in one shot, directly from the original images. It's a shortcut for the forward diffusion process.\n",
    "* The target is the noise that was used to produce the noisy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(X):\n",
    "    X = tf.cast(X[..., tf.newaxis], tf.float32) * 2 - 1  # scale from –1 to +1\n",
    "    X_shape = tf.shape(X)\n",
    "    t = tf.random.uniform([X_shape[0]], minval=1, maxval=T + 1, dtype=tf.int32)\n",
    "    alpha_cm = tf.gather(alpha_cumprod, t)\n",
    "    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n",
    "    noise = tf.random.normal(X_shape)\n",
    "    return {\n",
    "        \"X_noisy\": alpha_cm ** 0.5 * X + (1 - alpha_cm) ** 0.5 * noise,\n",
    "        \"time\": t,\n",
    "    }, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare a `tf.data.Dataset` for training, and one for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9qt5HPZ8rhv"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(X, batch_size=32, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(X)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10_000)\n",
    "    return ds.batch(batch_size).map(prepare_batch).prefetch(1)\n",
    "\n",
    "tf.random.set_seed(43)  # extra code – ensures reproducibility on CPU\n",
    "train_set = prepare_dataset(X_train, batch_size=32, shuffle=True)\n",
    "valid_set = prepare_dataset(X_valid, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAxL8EmF3Hcl"
   },
   "source": [
    "As a quick sanity check, let's take a look at a few training samples, along with the corresponding noise to predict, and the original images (which we get by subtracting the appropriately scaled noise from the appropriately scaled noisy image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "y8mwYDDtz6Zm",
    "outputId": "e5187d06-3d7b-4ab8-8b29-dc57c3428964"
   },
   "outputs": [],
   "source": [
    "# extra code – just a quick sanity check\n",
    "\n",
    "def subtract_noise(X_noisy, time, noise):\n",
    "    X_shape = tf.shape(X_noisy)\n",
    "    alpha_cm = tf.gather(alpha_cumprod, time)\n",
    "    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n",
    "    return (X_noisy - (1 - alpha_cm) ** 0.5 * noise) / alpha_cm ** 0.5\n",
    "\n",
    "X_dict, Y_noise = list(train_set.take(1))[0]  # get the first batch\n",
    "X_original = subtract_noise(X_dict[\"X_noisy\"], X_dict[\"time\"], Y_noise)\n",
    "\n",
    "print(\"Original images\")\n",
    "plot_multiple_images(X_original[:8].numpy())\n",
    "plt.show()\n",
    "print(\"Time steps:\", X_dict[\"time\"].numpy()[:8])\n",
    "print(\"Noisy images\")\n",
    "plot_multiple_images(X_dict[\"X_noisy\"][:8].numpy())\n",
    "plt.show()\n",
    "print(\"Noise to predict\")\n",
    "plot_multiple_images(Y_noise[:8].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcBpIkNxrPw9"
   },
   "source": [
    "Now we're ready to build the diffusion model itself. It will need to process both images and times. We will encode the times using a sinusoidal encoding, as suggested in the DDPM paper, just like in the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper. Given a vector of _m_ integers representing time indices (integers), the layer returns an _m_ × _d_ matrix, where _d_ is the chosen embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HGLhS4oNkNC"
   },
   "outputs": [],
   "source": [
    "# extra code – implements a custom time encoding layer\n",
    "\n",
    "embed_size = 64\n",
    "\n",
    "class TimeEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, T, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
    "        p, i = np.meshgrid(np.arange(T + 1), 2 * np.arange(embed_size // 2))\n",
    "        t_emb = np.empty((T + 1, embed_size))\n",
    "        t_emb[:, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        t_emb[:, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.time_encodings = tf.constant(t_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.gather(self.time_encodings, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18SYrUNysJ62"
   },
   "source": [
    "Now let's build the model. In the Improved DDPM paper, they use a UNet model. We'll create a UNet-like model, that processes the image through `Conv2D` + `BatchNormalization` layers and skip connections, gradually downsampling the image (using `MaxPooling` layers with `strides=2`), then growing it back again (using `Upsampling2D` layers). Skip connections are also added across the downsampling part and the upsampling part. We also add the time encodings to the output of each block, after passing them through a `Dense` layer to resize them to the right dimension.\n",
    "\n",
    "* **Note**: an image's time encoding is added to every pixel in the image, along the last axis (channels). So the number of units in the `Conv2D` layer must correspond to the embedding size, and we must reshape the `time_enc` tensor to add the width and height dimensions.\n",
    "* This UNet implementation was inspired by keras.io's [image segmentation example](https://keras.io/examples/vision/oxford_pets_image_segmentation/), as well as from the [official diffusion models implementation](https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/models/unet.py). Compared to the first implementation, I added a few things, especially time encodings and skip connections across down/up parts. Compared to the second implementation, I removed a few things, especially the attention layers. It seemed like overkill for Fashion MNIST, but feel free to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QLnc8wkfXrh"
   },
   "outputs": [],
   "source": [
    "def build_diffusion_model():\n",
    "    X_noisy = tf.keras.layers.Input(shape=[28, 28, 1], name=\"X_noisy\")\n",
    "    time_input = tf.keras.layers.Input(shape=[], dtype=tf.int32, name=\"time\")\n",
    "    time_enc = TimeEncoding(T, embed_size)(time_input)\n",
    "\n",
    "    dim = 16\n",
    "    Z = tf.keras.layers.ZeroPadding2D((3, 3))(X_noisy)\n",
    "    Z = tf.keras.layers.Conv2D(dim, 3)(Z)\n",
    "    Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "    Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "\n",
    "    time = tf.keras.layers.Dense(dim)(time_enc)  # adapt time encoding\n",
    "    Z = time[:, tf.newaxis, tf.newaxis, :] + Z  # add time data to every pixel\n",
    "\n",
    "    skip = Z\n",
    "    cross_skips = []  # skip connections across the down & up parts of the UNet\n",
    "\n",
    "    for dim in (32, 64, 128):\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        cross_skips.append(Z)\n",
    "        Z = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(Z)\n",
    "        skip_link = tf.keras.layers.Conv2D(dim, 1, strides=2,\n",
    "                                           padding=\"same\")(skip)\n",
    "        Z = tf.keras.layers.add([Z, skip_link])\n",
    "\n",
    "        time = tf.keras.layers.Dense(dim)(time_enc)\n",
    "        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n",
    "        skip = Z\n",
    "\n",
    "    for dim in (64, 32, 16):\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        Z = tf.keras.layers.UpSampling2D(2)(Z)\n",
    "\n",
    "        skip_link = tf.keras.layers.UpSampling2D(2)(skip)\n",
    "        skip_link = tf.keras.layers.Conv2D(dim, 1, padding=\"same\")(skip_link)\n",
    "        Z = tf.keras.layers.add([Z, skip_link])\n",
    "\n",
    "        time = tf.keras.layers.Dense(dim)(time_enc)\n",
    "        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n",
    "        Z = tf.keras.layers.concatenate([Z, cross_skips.pop()], axis=-1)\n",
    "        skip = Z\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, 3, padding=\"same\")(Z)[:, 2:-2, 2:-2]\n",
    "    return tf.keras.Model(inputs=[X_noisy, time_input], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R60NMcUT0b1K"
   },
   "source": [
    "Let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5chylX-Vp9te",
    "outputId": "fdd4f9f1-801e-4830-c8db-c8983f444ad8"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on the CPU\n",
    "model = build_diffusion_model()\n",
    "model.compile(loss=tf.keras.losses.Huber(), optimizer=\"nadam\")\n",
    "\n",
    "# extra code – adds a ModelCheckpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_diffusion_model\",\n",
    "                                                   save_best_only=True)\n",
    "\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=100,\n",
    "                    callbacks=[checkpoint_cb])  # extra code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can use it to generate new images. For this, we just generate Gaussian noise, and pretend this is the result of the diffusion process, and we're at time $T$. Then we use the model to predict the image at time $T - 1$, then we call it again to get $T - 2$, and so on, removing a bit of noise at each step. At the end, we get an image that looks like it's from the Fashion MNIST dataset. The equation for this reverse process is at the top of page 4 in the DDPM paper (step 4 in algorithm 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMXxz4qV8Luk"
   },
   "outputs": [],
   "source": [
    "def generate(model, batch_size=32):\n",
    "    X = tf.random.normal([batch_size, 28, 28, 1])\n",
    "    for t in range(T - 1, 0, -1):\n",
    "        print(f\"\\rt = {t}\", end=\" \")  # extra code – show progress\n",
    "        noise = (tf.random.normal if t > 1 else tf.zeros)(tf.shape(X))\n",
    "        X_noise = model({\"X_noisy\": X, \"time\": tf.constant([t] * batch_size)})\n",
    "        X = (\n",
    "            1 / alpha[t] ** 0.5\n",
    "            * (X - beta[t] / (1 - alpha_cumprod[t]) ** 0.5 * X_noise)\n",
    "            + (1 - alpha[t]) ** 0.5 * noise\n",
    "        )\n",
    "    return X\n",
    "\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on the CPU\n",
    "X_gen = generate(model)  # generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "lfMa2NVwJuzE",
    "outputId": "fc827f13-0b57-4372-a385-4cf70349ebd3"
   },
   "outputs": [],
   "source": [
    "plot_multiple_images(X_gen.numpy(), 8)\n",
    "save_fig(\"ddpm_generated_images_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these images are really convincing! Compared to GANs, diffusion models tend to generate more diverse images, and they have surpassed GANs in image quality. Moreover, training is much more stable. However, generating images takes *much* longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKyC0R9dyLhV"
   },
   "source": [
    "# Extra Material – Hashing Using a Binary Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDE4f-XpyLhW"
   },
   "source": [
    "Let's train an autoencoder where the encoder has a 16-neuron output layer, using the sigmoid activation function, and heavy Gaussian noise just before it. During training, the noise layer will encourage the previous layer to output large values, since small values will just be crushed by the noise. In turn, this means that the output layer will output values close to 0 or 1, thanks to the sigmoid activation function. Once we round the output values to 0s and 1s, we get a 16-bit \"semantic\" hash. If everything works well, images that look alike will have the same hash. This can be very useful for search engines: for example, if we store each image on a server identified by the image's semantic hash, then all similar images will end up on the same server. Users of the search engine can then provide an image to search for, and the search engine will compute the image's hash using the encoder, and quickly return all the images on the server identified by that hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DRvS7twyLhW",
    "outputId": "80a63cbf-0f33-4b6c-8e25-32474da2e2fb"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "hashing_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.GaussianNoise(15.),\n",
    "    tf.keras.layers.Dense(16, activation=\"sigmoid\"),\n",
    "])\n",
    "hashing_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "hashing_ae = tf.keras.Sequential([hashing_encoder, hashing_decoder])\n",
    "hashing_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = hashing_ae.fit(X_train, X_train, epochs=10,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OpQ1O-qyLhW"
   },
   "source": [
    "The autoencoder compresses the information so much (down to 16 bits!) that it's quite lossy, but that's okay, we're using it to produce semantic hashes, not to perfectly reconstruct the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyOsfAdiyLhW",
    "outputId": "678cc611-c625-434e-90b9-ad90d88baef4"
   },
   "outputs": [],
   "source": [
    "plot_reconstructions(hashing_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8Q_Rx-PyLhX"
   },
   "source": [
    "Now let's see what the hashes look like for the first few images in the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Akb6fNptyLhX",
    "outputId": "efdbf0bd-1506-42ae-f1e1-7cd68d0f45a7"
   },
   "outputs": [],
   "source": [
    "hashes = hashing_encoder.predict(X_valid).round().astype(np.int32)\n",
    "hashes *= np.array([[2 ** bit for bit in range(16)]])\n",
    "hashes = hashes.sum(axis=1)\n",
    "for h in hashes[:5]:\n",
    "    print(f\"{h:016b}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5n8N4RoyLhX"
   },
   "source": [
    "Now let's find the most common image hashes in the validation set, and display a few images for each hash. In the following image, all the images on a given row have the same hash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS80DfcLyLhX",
    "outputId": "51a38569-f02e-4737-d40e-b478d8a557b3"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "n_hashes = 10\n",
    "n_images = 8\n",
    "\n",
    "top_hashes = Counter(hashes).most_common(n_hashes)\n",
    "\n",
    "plt.figure(figsize=(n_images, n_hashes))\n",
    "for hash_index, (image_hash, hash_count) in enumerate(top_hashes):\n",
    "    indices = (hashes == image_hash)\n",
    "    for index, image in enumerate(X_valid[indices][:n_images]):\n",
    "        plt.subplot(n_hashes, n_images, hash_index * n_images + index + 1)\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J77AxVz5yLhX"
   },
   "source": [
    "# Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIf1WVKcyLhX"
   },
   "source": [
    "## 1. to 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSUWHPHUyLhX"
   },
   "source": [
    "1. Here are some of the main tasks that autoencoders are used for:\n",
    "    * Feature extraction\n",
    "    * Unsupervised pretraining\n",
    "    * Dimensionality reduction\n",
    "    * Generative models\n",
    "    * Anomaly detection (an autoencoder is generally bad at reconstructing outliers)\n",
    "2. If you want to train a classifier and you have plenty of unlabeled training data but only a few thousand labeled instances, then you could first train a deep autoencoder on the full dataset (labeled + unlabeled), then reuse its lower half for the classifier (i.e., reuse the layers up to the codings layer, included) and train the classifier using the labeled data. If you have little labeled data, you probably want to freeze the reused layers when training the classifier.\n",
    "3. The fact that an autoencoder perfectly reconstructs its inputs does not necessarily mean that it is a good autoencoder; perhaps it is simply an overcomplete autoencoder that learned to copy its inputs to the codings layer and then to the outputs. In fact, even if the codings layer contained a single neuron, it would be possible for a very deep autoencoder to learn to map each training instance to a different coding (e.g., the first instance could be mapped to 0.001, the second to 0.002, the third to 0.003, and so on), and it could learn \"by heart\" to reconstruct the right training instance for each coding. It would perfectly reconstruct its inputs without really learning any useful pattern in the data. In practice such a mapping is unlikely to happen, but it illustrates the fact that perfect reconstructions are not a guarantee that the autoencoder learned anything useful. However, if it produces very bad reconstructions, then it is almost guaranteed to be a bad autoencoder. To evaluate the performance of an autoencoder, one option is to measure the reconstruction loss (e.g., compute the MSE, or the mean square of the outputs minus the inputs). Again, a high reconstruction loss is a good sign that the autoencoder is bad, but a low reconstruction loss is not a guarantee that it is good. You should also evaluate the autoencoder according to what it will be used for. For example, if you are using it for unsupervised pretraining of a classifier, then you should also evaluate the classifier's performance.\n",
    "4. An undercomplete autoencoder is one whose codings layer is smaller than the input and output layers. If it is larger, then it is an overcomplete autoencoder. The main risk of an excessively undercomplete autoencoder is that it may fail to reconstruct the inputs. The main risk of an overcomplete autoencoder is that it may just copy the inputs to the outputs, without learning any useful features.\n",
    "5. To tie the weights of an encoder layer and its corresponding decoder layer, you simply make the decoder weights equal to the transpose of the encoder weights. This reduces the number of parameters in the model by half, often making training converge faster with less training data and reducing the risk of overfitting the training set.\n",
    "6. A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic images of digits. The output distribution is typically similar to the training data. For example, since MNIST contains many images of each digit, the generative model would output roughly the same number of images of each digit. Some generative models can be parametrized—for example, to generate only some kinds of outputs. An example of a generative autoencoder is the variational autoencoder.\n",
    "7. A generative adversarial network is a neural network architecture composed of two parts, the generator and the discriminator, which have opposing objectives. The generator's goal is to generate instances similar to those in the training set, to fool the discriminator. The discriminator must distinguish the real instances from the generated ones. At each training iteration, the discriminator is trained like a normal binary classifier, then the generator is trained to maximize the discriminator's error. GANs are used for advanced image processing tasks such as super resolution, colorization, image editing (replacing objects with realistic background), turning a simple sketch into a photorealistic image, or predicting the next frames in a video. They are also used to augment a dataset (to train other models), to generate other types of data (such as text, audio, and time series), and to identify the weaknesses in other models and strengthen them.\n",
    "8. Training GANs is notoriously difficult, because of the complex dynamics between the generator and the discriminator. The biggest difficulty is mode collapse, where the generator produces outputs with very little diversity. Moreover, training can be terribly unstable: it may start out fine and then suddenly start oscillating or diverging, without any apparent reason. GANs are also very sensitive to the choice of hyperparameters.\n",
    "9. Diffusion models are good at generating diverse and high quality images. They are also much easier to train than GANs. However, compared to GANs and VAEs, they are much slower when generating images, since they must go through each step in the reverse diffusion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GXqpaDHyLhY"
   },
   "source": [
    "## 10.\n",
    "_Exercise: Try using a denoising autoencoder to pretrain an image classifier. You can use MNIST (the simplest option), or a more complex image dataset such as [CIFAR10](https://homl.info/122) if you want a bigger challenge. Regardless of the dataset you're using, follow these steps:_\n",
    "* Split the dataset into a training set and a test set. Train a deep denoising autoencoder on the full training set.\n",
    "* Check that the images are fairly well reconstructed. Visualize the images that most activate each neuron in the coding layer.\n",
    "* Build a classification DNN, reusing the lower layers of the autoencoder. Train it using only 500 images from the training set. Does it perform better with or without pretraining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVAVmIznyLhY"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXcft-kHyLhY"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "denoising_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.GaussianNoise(0.1),\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2lC7XBqyLhY"
   },
   "outputs": [],
   "source": [
    "denoising_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16 * 16 * 32, activation=\"relu\"),\n",
    "    tf.keras.layers.Reshape([16, 16, 32]),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=2,\n",
    "                                 padding=\"same\", activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgtxA9BjyLhY",
    "outputId": "7b43827a-cc75-45cd-e5e0-dbd4ccfea440"
   },
   "outputs": [],
   "source": [
    "denoising_ae = tf.keras.Sequential([denoising_encoder, denoising_decoder])\n",
    "denoising_ae.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "                     metrics=[\"mse\"])\n",
    "history = denoising_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTlFyrmSyLhY",
    "outputId": "7a842325-f99e-4e85-962f-ba3bcbf8cdfd"
   },
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "new_images = X_test[:n_images]\n",
    "new_images_noisy = new_images + np.random.randn(n_images, 32, 32, 3) * 0.1\n",
    "new_images_denoised = denoising_ae.predict(new_images_noisy)\n",
    "\n",
    "plt.figure(figsize=(6, n_images * 2))\n",
    "for index in range(n_images):\n",
    "    plt.subplot(n_images, 3, index * 3 + 1)\n",
    "    plt.imshow(new_images[index])\n",
    "    plt.axis('off')\n",
    "    if index == 0:\n",
    "        plt.title(\"Original\")\n",
    "    plt.subplot(n_images, 3, index * 3 + 2)\n",
    "    plt.imshow(new_images_noisy[index].clip(0., 1.))\n",
    "    plt.axis('off')\n",
    "    if index == 0:\n",
    "        plt.title(\"Noisy\")\n",
    "    plt.subplot(n_images, 3, index * 3 + 3)\n",
    "    plt.imshow(new_images_denoised[index])\n",
    "    plt.axis('off')\n",
    "    if index == 0:\n",
    "        plt.title(\"Denoised\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddB3aKyFyLhZ"
   },
   "source": [
    "## 11.\n",
    "_Exercise: Train a variational autoencoder on the image dataset of your choice, and use it to generate images. Alternatively, you can try to find an unlabeled dataset that you are interested in and see if you can generate new samples._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEfVD1TTyLhZ"
   },
   "source": [
    "See the VAE code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mePWI8ryLhZ"
   },
   "source": [
    "## 12.\n",
    "_Exercise: Train a DCGAN to tackle the image dataset of your choice, and use it to generate images. Add experience replay and see if this helps. Turn it into a conditional GAN where you can control the generated class._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWBNDAFvyLhZ"
   },
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "17_autoencoders_gans_and_diffusion_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nav_menu": {
   "height": "381px",
   "width": "453px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
